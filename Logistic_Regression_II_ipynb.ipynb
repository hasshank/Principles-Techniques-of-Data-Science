{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA7ZHYPUfrtd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Vivid\n",
        "px.defaults.width = 800\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import sklearn.linear_model as lm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basketball = pd.read_csv(\"data/nba.csv\")\n",
        "first_team = basketball.groupby(\"GAME_ID\").first()\n",
        "second_team = basketball.groupby(\"GAME ID\").last()\n",
        "games = first_team.merge(second_team, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
        "games['GOAL_DIFF'] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
        "games['WON'] = (games['WL'] == \"W\").astype(int)\n",
        "games = games[['TEAM_NAME', 'TEAM_NAME_OPP', 'MATCHUP', 'WON', 'WL', 'AST', 'GOAL_DIFF']]\n",
        "games"
      ],
      "metadata": {
        "id": "i---36nKgZ12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "games[\"JitterWON\"] = games[\"WON\"] = np.random.uniform(-0.1, 0.1, len(games))\n",
        "px.scatter(games, x=\"GOAL_DIFF\", y=\"JitterWON\", color=\"WL\")"
      ],
      "metadata": {
        "id": "QbhZWl8Mh9Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = games[[\"GOAL_DIFF\"]]\n",
        "Y = games[\"WON\"]\n",
        "\n",
        "model = lm.LogisticRegression()\n",
        "model.fit(X, Y)\n",
        "print(\"Slope:\", model.coef_[0][0])\n",
        "print(\"Intercept:\", model.intercept_[0])"
      ],
      "metadata": {
        "id": "LNcULlZ-iAun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_proba(X)[:10]"
      ],
      "metadata": {
        "id": "UWkLbCJ-iZlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classes_"
      ],
      "metadata": {
        "id": "mR-Ev-T7igfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict_proba(X)[:, 1]\n",
        "\n",
        "(p >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "ZJoWrox-i3_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = model.predict(X)\n",
        "\n",
        "classes"
      ],
      "metadata": {
        "id": "PxRuDSVRjALr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-model.intercept_[0]/model.coef_[0][0]"
      ],
      "metadata": {
        "id": "AEbrLe9SjDdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games[\"Predicted Class\"] = pd.Categorical(classes)\n",
        "\n",
        "test_points = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
        "test_points[\"Predicted Prob\"] = model.predict_proba(test_points)[:, 1]\n",
        "\n",
        "fig = px.scatter(games, x=\"GOAL_DIFF\", y=\"JitterWON\", color=\"Predicted Class\")\n",
        "fig.add_trace(go.Scatter(x=test_points[\"GOAL_DIFF\"], y=test_points[\"Predicted Prob\"],\n",
        "                         mode=\"lines\", name=\"Logistic Regression Model\",\n",
        "                         line_color=\"black\", line_width=5, line_dash=\"dash\"))\n",
        "fig.add_vline(x= -model.intercept_[0]/model.coef_[0][0], line_dash=\"dash\",\n",
        "              line_color=\"black\",\n",
        "              annotation_text=\"Decision Boundary\",\n",
        "              annotation_position=\"right\")"
      ],
      "metadata": {
        "id": "KlTsIwnjjJBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(games, x=\"GOAL_DIFF\", y=np.zeros(len(games)),\n",
        "                 symbol=\"WL\", symbol_sequence=[\"circle-open\", \"cross\"],\n",
        "                 color=\"Predicted Class\", height=300, opacity=0.7)\n",
        "fig.update_traces(marker_size=8)\n",
        "fig.update_layout(\n",
        "    yaxis=dict(showticklabels=False, showgrid=False, zeroline=False, title=\"\"),\n",
        ")\n",
        "\n",
        "decision_boudary = -model.intercept_[0]/model.coef_[0][0]\n",
        "fig.add_vline(x= decision_boudary, line_dash=\"dash\",\n",
        "              line_color=\"black\",\n",
        "              annotation_text=\"Decision Boundary\",\n",
        "              annotation_position=\"top right\")"
      ],
      "metadata": {
        "id": "z38etJ28krGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_two_feature = games[[\"GOAL_DIFF\", \"AST\"]]\n",
        "Y = games[\"WON\"]\n",
        "\n",
        "two_feature_model = lm.LogisticRegression()\n",
        "two_feature_model.fit(X_two_feature, Y)\n",
        "\n",
        "theta0 = two_feature_model.intercept_\n",
        "theta1, theta2 = two_feature_model.coef_[0]\n",
        "print(theta0, theta1, theta2)"
      ],
      "metadata": {
        "id": "W5XTl40-obqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games[\"Predicted Class\"] = two_feature_model.predict(X_two_feature)\n",
        "games.head()"
      ],
      "metadata": {
        "id": "0UUl2VQvo-tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_boudary = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
        "decision_boudary[\"AST\"] = (theta0 + theta1*decision_boudary[\"GOAL_DIFF\"])/(-theta2)"
      ],
      "metadata": {
        "id": "sKEFcfdapNRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games['Predicted Class'] = pd.Categorical(games['Predicted Class'])\n",
        "fig = px.scatter(games, x=\"GOAL_DIFF\", y=\"AST\", symbol=\"WL\",\n",
        "                 hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'],\n",
        "                 color=\"Predicted Class\",\n",
        "                 symbol_sequence=[\"circle-open\", \"cross\"],\n",
        "                 opacity=0.7,\n",
        "                 height=600)\n",
        "fig.update_traces(marker=dict(size=8))\n",
        "fig.update_layout(xaxis_range=[-0.3, 0.3], yaxis_range=[5, 50])\n",
        "fig.add_scatter(x=decision_boudary[\"GOAL_DIFF\"], y=decision_boudary[\"AST\"],\n",
        "                mode=\"lines\", line_color=\"black\", line_dash=\"dash\",\n",
        "                name=\"Decision Boudary\")"
      ],
      "metadata": {
        "id": "S6GT0EjbpuhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goal_diff, ast = np.meshgrid(np.linspace(-0.3, 0.3, 50), np.linspace(5, 50, 50))\n",
        "pred_grid = pd.DataFrame({\"GOAL_DIFF\": np.ravel(goal_diff), \"AST\": np.ravel(ast)})\n",
        "pred_grid['Probability'] = two_feature_model.predict_proba(pred_grid)[:, 1]\n",
        "\n",
        "fig.add_contour(x=pred_grid['GOAL_DIFF'], y=pred_grid['AST'], z=pred_grid['Probability'],\n",
        "                showscale=False, opacity=0.4, colorscale=\"Matter\")"
      ],
      "metadata": {
        "id": "jqcfj_lushmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "iris = sns.load_dataset(\"iris\")"
      ],
      "metadata": {
        "id": "FOQNZ9wuj4YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(iris[iris[\"species\"] != \"virginica\"],\n",
        "                 x = \"petal_length\",\n",
        "                 y = \"petal_width\",\n",
        "                 color = \"species\",\n",
        "                 symbol = \"species\", symbol_sequence=[\"circle\", \"cross\"],\n",
        "                 render_mode=\"svg\")\n",
        "fig.update_traces(marker=dict(size=12))\n",
        "fig"
      ],
      "metadata": {
        "id": "XRe-dO3zk4ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(iris[iris[\"species\"] != \"setosa\"],\n",
        "                 x = \"petal_length\",\n",
        "                 y = \"petal_width\",\n",
        "                 color = \"species\",\n",
        "                 symbol = \"species\", symbol_sequence=[\"circle\", \"cross\"],\n",
        "                 render_mode= \"svg\")\n",
        "fig.update_traces(marker=dict(size=12))\n",
        "fig"
      ],
      "metadata": {
        "id": "y4k6azx9kXbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_df = pd.DataFrame({\"x\": [-1, 1], \"y\": [0, 1], \"label\": pd.Categorical([0,1])})\n",
        "fig = px.scatter(toy_df, x=\"x\", y=\"y\",\n",
        "                 color=\"label\", symbol=\"label\",\n",
        "                 symbol_sequence=[\"circle\", \"cross\"],\n",
        "                 render_mode=\"svg\")\n",
        "fig.update_traces(marker=dict(size=12))"
      ],
      "metadata": {
        "id": "LjU_GFs3k6uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toy_model(theta1, x):\n",
        "  return 1/(1 + np.exp(-theta1 * x))\n",
        "\n",
        "def mean_cross_entropy_loss_toy(theta1):\n",
        "  return -np.sum(toy_df['y'] * np.log(toy_model(theta1, toy_df['x'])) + \\\n",
        "                 (1-toy_df['y']) * np.log(toy_model(theta1, -toy_df['x'])))"
      ],
      "metadata": {
        "id": "Fwc4LVy5l4AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thetas = np.linspace(-30, 30, 100)\n",
        "fig = px.line(x=thetas, y = [mean_cross_entropy_loss_toy(theta) for theta in thetas],\n",
        "              render_mode=\"svg\")\n",
        "fig.update_layout(xaxis_title=\"Theta\", yaxis_title=\"Mean CE Loss\",\n",
        "                  title = \"Mean Cross Entropy Loss for Toy Example\")"
      ],
      "metadata": {
        "id": "aUFkQzSKmXOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(x=thetas, y= [mean_cross_entropy_loss_toy(theta) for theta in thetas],\n",
        "              log_y=True, render_mode=\"svg\")\n",
        "fig.update_layout(xaxis_title=\"Theta\", yaxis_title=\"Log Scale Mean CE Loss\",\n",
        "                  title=\"Log Scale Mean Cross Entropy Loss for Toy Example\")"
      ],
      "metadata": {
        "id": "UIuYnehimrcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regularized_loss_toy(theta1, reg):\n",
        "  return mean_cross_entropy_loss_toy(theta1) + reg * theta1**2"
      ],
      "metadata": {
        "id": "bdgX_p2bnBAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = 0.01\n",
        "fig = px.line(x=thetas, y= [regularized_loss_toy(theta, reg) for theta in thetas],\n",
        "              render_mode = \"svg\")\n",
        "fig.update_layout(xaxis_title = \"Theta\", yaxis_title = \"Mean CE Loss\",\n",
        "                  title = f\"Mean Cross Entropy Loss for Toy Example (Regularization = {reg})\")"
      ],
      "metadata": {
        "id": "HBltcTZKnJXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_model = lm.LogisticRegression(C=10)\n",
        "toy_model.fit([[-1], [1]], [0,1])\n",
        "\n",
        "xtest = np.linspace(-1.5, 1.5, 1000)[:, np.newaxis]\n",
        "p = toy_model.predict_proba(xtest)[:,1]\n",
        "\n",
        "fig = px.scatter(toy_df, x=\"x\", y=\"y\",\n",
        "                 color=\"label\", symbol=\"label\",\n",
        "                 symbol_sequence=[\"circle\", \"cross\"],\n",
        "                 title = f\"LR fit (slope = {model.coef_[0][0]}, intercept = {model.intercept_[0]})\",\n",
        "                 render_mode=\"svg\")\n",
        "fig.add_scatter(x=np.ravel(xtest), y=p, mode=\"lines\", name=\"Logistic Regression Model\",\n",
        "                line_color=\"black\", line_width=5, line_dash=\"dash\")"
      ],
      "metadata": {
        "id": "JMyQgAw2n5wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_model = lm.LogisticRegression(C=1000)\n",
        "toy_model.fit([[-1], [1]], [0,1])\n",
        "\n",
        "xtest = np.linspace(-1.5, 1.5, 1000)[:, np.newaxis]\n",
        "p = toy_model.predict_proba(xtest)[:, 1]\n",
        "\n",
        "fig = px.scatter(toy_df, x=\"x\", y=\"y\",\n",
        "                 color=\"label\", symbol=\"label\",\n",
        "                 symbol_sequence=[\"circle\", \"cross\"],\n",
        "                 title=f\"LR Fit (slope = {model.coef_[0][0]}, intercept = {model.intercept_[0]})\",\n",
        "                 render_mode=\"svg\")\n",
        "fig.add_scatter(x=np.ravel(xtest), y=p, mode=\"lines\", name=\"Logistic Regression Model\",\n",
        "                line_color=\"black\", line_width=5, line_dash=\"dash\")"
      ],
      "metadata": {
        "id": "WXZ4lKhfpDqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(X, Y):\n",
        "  return np.mean(model.predict(X) == Y)\n",
        "\n",
        "accuracy(X, Y)"
      ],
      "metadata": {
        "id": "wZlntk0BtZ8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X, Y)"
      ],
      "metadata": {
        "id": "EJ2dACAQtgX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(Y, model.predict(X))\n",
        "cm"
      ],
      "metadata": {
        "id": "ehokdEjKtlZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.imshow(cm, x=[\"0\", \"1\"], y=[\"0\", \"1\"],\n",
        "                labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
        "                text_auto=True,\n",
        "                color_continuous_scale=\"Blues\",\n",
        "                width=400, height=400)\n",
        "fig.update_xaxes(side=\"top\")"
      ],
      "metadata": {
        "id": "KB4M6yVmtsdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_hat = model.predict(X)\n",
        "tp = np.sum((Y_hat == 1) & (Y == 1))\n",
        "tn = np.sum((Y_hat == 0) & (Y == 0))\n",
        "\n",
        "fp = np.sum((Y_hat == 1) & (Y == 0))\n",
        "fn = np.sum((Y_hat == 0) & (Y == 1))\n",
        "\n",
        "print(\"True Positives: \", tp)\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)"
      ],
      "metadata": {
        "id": "XLk0SxdjuDSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = tp / (tp + fp)\n",
        "precision"
      ],
      "metadata": {
        "id": "UipPyGnrviZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall = tp / (tp + fn)\n",
        "recall"
      ],
      "metadata": {
        "id": "-bxdRUF0vwYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr = fp/(fp + tn)\n",
        "fpr"
      ],
      "metadata": {
        "id": "080H5EWQvz3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpr = tp/(tp + fn)\n",
        "tpr"
      ],
      "metadata": {
        "id": "tVmnxVOGv3M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = games[[\"GOAL_DIFF\"]]\n",
        "Y = games[\"WON\"]\n",
        "model = lm.LogisticRegression()\n",
        "model.fit(X, Y)\n",
        "print(\"Slope:\", model.coef_[0][0])\n",
        "print(\"Intercept:\", model.intercept_[0])"
      ],
      "metadata": {
        "id": "cRkWkOeAv_ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predicitions(threshold = 0.5):\n",
        "  games[\"Predicted Class\"] = model.predict_proba(X)[:, 1] >= threshold\n",
        "  games[\"Predicted Class\"] = pd.Categorical(games[\"Predicted Class\"])\n",
        "  fig = px.scatter(games,\n",
        "                   x=\"GOAL_DIFF\", y=\"JitterWON\", color=\"Predicted Class\",\n",
        "                   title=f\"Logistic Regression Predictions (Threshold = {threshold})\")\n",
        "  test_points = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
        "  test_points[\"Predicted Prob\"] = model.predict_proba(test_points)[:, 1]\n",
        "  fig.add_trace(go.Scatter(x=test_points[\"GOAL_DIFF\"], y=test_points[\"Predicted Prob\"],\n",
        "                           mode=\"lines\", name=\"Logistic Regression Model\",\n",
        "                           line_color=\"black\", line_width=5, line_dash=\"dash\"))\n",
        "  decision_boundary = (-np.log(1/threshold - 1) - model.intercept_[0])/model.coef_[0][0]\n",
        "  fig.add_value(x=decision_boudary, line_dash=\"dash\", line_color=\"black\",\n",
        "                annotation_text=\"Decision Boundary\", annotation_position=\"right\")\n",
        "  return fig\n",
        "\n",
        "plot_predictions(0.5)"
      ],
      "metadata": {
        "id": "eEfVOpcwwVGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(0.25)"
      ],
      "metadata": {
        "id": "IcYtB0LQ8RsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(0.75)"
      ],
      "metadata": {
        "id": "uwYj4Lrk8XL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_threshold(model, X, T):\n",
        "  prob_one = model.predict_proba(X)[:, 1]\n",
        "  return (prob_one >= T).astype(int)\n",
        "\n",
        "def accuracy_threshold(X, Y, T):\n",
        "  return np.mean(predict_threshold(model, X, T) == Y)\n",
        "\n",
        "def precision_treshold(Y, Y, T):\n",
        "  Y_hat = predict_threshold(model, X, T)\n",
        "  denominator = np.sum(Y_hat == 1)\n",
        "  if denominator == 0:\n",
        "    denominator = np.nan\n",
        "    return np.sum((Y_hat == 1) & (Y == 1)) / denominator\n",
        "\n",
        "def recall_threshold(X, Y, T):\n",
        "  Y_hat = predict_threshold(model, X, T)\n",
        "  return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y == 1)\n",
        "\n",
        "def tpr_threshold(X, Y, T):\n",
        "  Y_hat = predict_threshold(model, X, T)\n",
        "  return np.sum((Y_hat == 1) & (Y == 1))/ np.sum(Y == 1)\n",
        "\n",
        "def fpr_threshold(X, Y, T):\n",
        "  Y_hat = predict_threshold(model, X, T)\n",
        "  return np.sum((Y_hat == 1) & (Y == 0)) / np.sum(Y == 0)"
      ],
      "metadata": {
        "id": "A041UvM98aec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = pd.DataFrame()\n",
        "metrics[\"Threshold\"] = np.linspace(0,1, 1000)\n",
        "metrics[\"Accuracy\"] = [accuracy_threshold(X, Y, t) for t in metric[\"Threshold\"]]\n",
        "metrics[\"Precision\"] = [precision_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
        "metrics[\"Recall\"] = [recall_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
        "metrics.head()"
      ],
      "metadata": {
        "id": "p-lznSLwClgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(metrics,\n",
        "        x=\"Threshold\", y=\"Accuracy\",\n",
        "        title=\"Accuracy vs. Threshold\",\n",
        "        render_mode=\"svg\")"
      ],
      "metadata": {
        "id": "xy5X_SudDSqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.sort_values(\"Accuracy\", ascending=False).head()"
      ],
      "metadata": {
        "id": "TpSTnY8tDcmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(metrics,\n",
        "        x=\"Threshold\", y=[\"Accuracy\", \"Precision\", \"Recall\"],\n",
        "        title=\"Performance Metrics vs. Threshold\",\n",
        "        render_mode=\"svg\")"
      ],
      "metadata": {
        "id": "dv7BdYyoDjDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(metrics, x=\"Recall\", y=\"Precision\",\n",
        "        title=\"Precision vs. Recall\",\n",
        "        width=600, height=600,\n",
        "        render_mode=\"svg\")"
      ],
      "metadata": {
        "id": "g_WHON5oDzNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics[\"F1\"] = (2 * metrics[\"Precision\"] * metrics[\"Recall\"]\n",
        "                 / (metrics[\"Precision\"] + metrics[\"Recall\"]))\n",
        "fig = px.line(metrics, x=\"Threshold\", y=\"F1\",\n",
        "              title=\"Finding F1 Score Maximum\",\n",
        "              render_mode=\"svg\")\n",
        "ind = metrics['F1'].idxmax()\n",
        "fig.add_scatter(x=[metrics.loc[ind, 'Threshold']], y=[metrics.loc[ind, 'F1']],\n",
        "                mode='markers', marker=dict(size=10, color='red'),\n",
        "                name=f\"F1 Max {metrics.loc[ind, 'Threshold']:.5f}\",)"
      ],
      "metadata": {
        "id": "n-rKvkNDEIln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(metrics, x=\"Recall\", y=\"Precision\",\n",
        "              title=\"Precision vs. Recall\", width = 600, height=600,\n",
        "              render_mode=\"svg\")\n",
        "fig.add_scatter(x=[metrics.loc[ind, 'Recall']], y=[metrics.loc[ind, 'Precision']],\n",
        "                mode='markers', marker=dict(size=10, color='red'),\n",
        "                name=f\"F1 Max {metrics.loc[ind, 'Threshold']:.5f}\")\n",
        "fig.update_layout(legend=dict(x=.5, y=.1))"
      ],
      "metadata": {
        "id": "d5SQFHD4FeZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics[\"TPR\"] = [tpr_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
        "metrics[\"FPR\"] = [tpr_threshold(X, Y, t) for t in metrics[\"Threshold\"]]"
      ],
      "metadata": {
        "id": "HoHaVjwqJKDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(metrics, x=\"Threshold\", y=[\"TPR\", \"FPR\", \"Accuracy\"],\n",
        "        render_mode=\"svg\")"
      ],
      "metadata": {
        "id": "-2FyEtReJyJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(metrics, x=\"FPR\", y=\"TPR\", title=\"ROC Curve\",\n",
        "        width=600, height=600,\n",
        "        render_mode=\"svg\")"
      ],
      "metadata": {
        "id": "ubt8I1odiRAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(metrics, x=\"FPR\", y=\"TPR\", title=\"ROC Curve\",\n",
        "              width=600, height=600,\n",
        "              render_mode=\"svg\")\n",
        "fig.add_scatter(x=[0,0,1], y=[0,1,1], mode='lines',\n",
        "                line_dash='dash', line_color='black',\n",
        "                name=\"Perfect Classifier\")\n",
        "fig.update_layout(legend=dict(x=.5, y=.1))"
      ],
      "metadata": {
        "id": "pJMjbXOFif4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}